{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPSci Latency Study - Statistical Analysis\n",
    "\n",
    "This notebook performs statistical analyses on the processed experimental data.\n",
    "\n",
    "## Analyses\n",
    "\n",
    "1. Descriptive statistics\n",
    "2. Correlation analysis (latency vs performance, latency vs QoE)\n",
    "3. Mixed-effects models (within-subjects repeated measures)\n",
    "4. Effect sizes and pairwise comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols, mixedlm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 observations\n",
      "Participants: 17\n",
      "Games: <StringArray>\n",
      "['Dave the Diver', 'Half-Life 2', 'Fitts Law', 'Feeding Frenzy',\n",
      " 'Rocket League']\n",
      "Length: 5, dtype: str\n",
      "Latency conditions: [np.float64(0.0), np.float64(75.0), np.float64(150.0), np.float64(225.0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>game</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>score</th>\n",
       "      <th>mean_time_ms</th>\n",
       "      <th>median_time_ms</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>throughput</th>\n",
       "      <th>n_trials</th>\n",
       "      <th>goals</th>\n",
       "      <th>shots</th>\n",
       "      <th>saves</th>\n",
       "      <th>assists</th>\n",
       "      <th>quality_rating</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>game_display</th>\n",
       "      <th>score_z</th>\n",
       "      <th>score_pct_of_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>-0.058850</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>-0.107517</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>-0.139961</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>-0.383293</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>half_life_2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Half-Life 2</td>\n",
       "      <td>0.789281</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id            game  latency_ms  score  mean_time_ms  \\\n",
       "0               1  dave_the_diver       150.0    3.0           NaN   \n",
       "1               1  dave_the_diver         0.0    2.7           NaN   \n",
       "2               1  dave_the_diver        75.0    2.5           NaN   \n",
       "3               1  dave_the_diver       225.0    1.0           NaN   \n",
       "4               1     half_life_2        75.0   15.0           NaN   \n",
       "\n",
       "   median_time_ms  error_rate  throughput  n_trials  goals  shots  saves  \\\n",
       "0             NaN         NaN         NaN       NaN    NaN    NaN    NaN   \n",
       "1             NaN         NaN         NaN       NaN    NaN    NaN    NaN   \n",
       "2             NaN         NaN         NaN       NaN    NaN    NaN    NaN   \n",
       "3             NaN         NaN         NaN       NaN    NaN    NaN    NaN   \n",
       "4             NaN         NaN         NaN       NaN    NaN    NaN    NaN   \n",
       "\n",
       "   assists  quality_rating  acceptable    game_display   score_z  \\\n",
       "0      NaN             4.0         1.0  Dave the Diver -0.058850   \n",
       "1      NaN             5.0         1.0  Dave the Diver -0.107517   \n",
       "2      NaN             5.0         1.0  Dave the Diver -0.139961   \n",
       "3      NaN             3.2         2.0  Dave the Diver -0.383293   \n",
       "4      NaN             4.5         1.0     Half-Life 2  0.789281   \n",
       "\n",
       "   score_pct_of_baseline  \n",
       "0             100.000000  \n",
       "1              90.000000  \n",
       "2              83.333333  \n",
       "3              33.333333  \n",
       "4             100.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../analysis/processed_data')\n",
    "OUTPUT_DIR = Path('../analysis/results')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load combined data\n",
    "df = pd.read_csv(DATA_DIR / 'combined_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} observations\")\n",
    "print(f\"Participants: {len(df['participant_id'].unique())}\")\n",
    "print(f\"Games: {df['game_display'].unique()}\")\n",
    "print(f\"Latency conditions: {sorted(df['latency_ms'].unique())}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY LATENCY CONDITION\n",
      "================================================================================\n",
      "           score                               score_z        quality_rating  \\\n",
      "           count     mean      std  min    max    mean    std           mean   \n",
      "latency_ms                                                                     \n",
      "0.0           76   94.610  232.591  0.0  970.0   0.514  1.059          4.533   \n",
      "75.0          75  129.327  273.575  0.0  970.0   0.313  0.819          3.749   \n",
      "150.0         75   92.669  187.747  0.0  980.0  -0.176  0.987          2.803   \n",
      "225.0         74   49.027   86.333  0.0  320.0  -0.667  0.602          2.043   \n",
      "\n",
      "                  acceptable         \n",
      "              std       mean    sum  \n",
      "latency_ms                           \n",
      "0.0         0.738      1.039   79.0  \n",
      "75.0        0.995      1.200   90.0  \n",
      "150.0       1.309      1.446  107.0  \n",
      "225.0       1.075      1.662  123.0  \n",
      "\n",
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY GAME\n",
      "================================================================================\n",
      "               score                                  quality_rating         \\\n",
      "               count     mean      std   min      max           mean    std   \n",
      "game_display                                                                  \n",
      "Dave the Diver    54    3.363    6.164  0.00   41.000          3.777  1.225   \n",
      "Feeding Frenzy    68  385.265  283.832  1.00  980.000          3.603  1.145   \n",
      "Fitts Law         64    3.476    1.183  1.65    5.993          2.619  1.515   \n",
      "Half-Life 2       64   12.484    3.187  2.00   19.000          3.041  1.530   \n",
      "Rocket League     50    1.340    1.154  0.00    4.000          3.538  1.248   \n",
      "\n",
      "               acceptable  \n",
      "                     mean  \n",
      "game_display               \n",
      "Dave the Diver      1.170  \n",
      "Feeding Frenzy      1.191  \n",
      "Fitts Law           1.562  \n",
      "Half-Life 2         1.438  \n",
      "Rocket League       1.280  \n",
      "\n",
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY GAME AND LATENCY\n",
      "================================================================================\n",
      "                          score                   quality_rating         \\\n",
      "                          count     mean      std           mean    std   \n",
      "game_display   latency_ms                                                 \n",
      "Dave the Diver 0.0           14    3.999    6.152          4.536  0.746   \n",
      "               75.0          13    2.877    1.022          4.231  0.857   \n",
      "               150.0         14    4.907   10.427          3.731  1.092   \n",
      "               225.0         13    1.500    0.852          2.554  1.206   \n",
      "Feeding Frenzy 0.0           17  399.765  354.974          4.459  0.850   \n",
      "               75.0          17  550.706  319.540          4.035  0.828   \n",
      "               150.0         17  390.588  202.314          3.394  1.002   \n",
      "               225.0         17  200.000   50.000          2.524  0.902   \n",
      "Fitts Law      0.0           16    5.083    0.541          4.800  0.400   \n",
      "               75.0          16    3.823    0.503          2.731  0.772   \n",
      "               150.0         16    2.780    0.384          1.725  0.790   \n",
      "               225.0         16    2.219    0.308          1.219  0.482   \n",
      "Half-Life 2    0.0           16   14.688    2.845          4.538  0.903   \n",
      "               75.0          16   13.500    2.503          3.738  0.981   \n",
      "               150.0         16   11.500    2.280          2.194  1.121   \n",
      "               225.0         16   10.250    3.235          1.694  1.090   \n",
      "Rocket League  0.0           13    1.692    1.182          4.292  0.670   \n",
      "               75.0          13    1.769    1.235          4.162  0.757   \n",
      "               150.0         12    1.083    1.165          3.208  1.445   \n",
      "               225.0         12    0.750    0.754          2.375  1.003   \n",
      "\n",
      "                          acceptable  \n",
      "                                mean  \n",
      "game_display   latency_ms             \n",
      "Dave the Diver 0.0             1.000  \n",
      "               75.0            1.077  \n",
      "               150.0           1.154  \n",
      "               225.0           1.462  \n",
      "Feeding Frenzy 0.0             1.000  \n",
      "               75.0            1.059  \n",
      "               150.0           1.118  \n",
      "               225.0           1.588  \n",
      "Fitts Law      0.0             1.000  \n",
      "               75.0            1.562  \n",
      "               150.0           1.812  \n",
      "               225.0           1.875  \n",
      "Half-Life 2    0.0             1.062  \n",
      "               75.0            1.188  \n",
      "               150.0           1.750  \n",
      "               225.0           1.750  \n",
      "Rocket League  0.0             1.154  \n",
      "               75.0            1.077  \n",
      "               150.0           1.333  \n",
      "               225.0           1.583  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY LATENCY CONDITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics by latency\n",
    "descriptive_latency = df.groupby('latency_ms').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'score_z': ['mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean', 'sum']\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_latency)\n",
    "\n",
    "# Save to file\n",
    "descriptive_latency.to_csv(OUTPUT_DIR / 'descriptive_stats_by_latency.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY GAME\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descriptive_game = df.groupby('game_display').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean']\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_game)\n",
    "descriptive_game.to_csv(OUTPUT_DIR / 'descriptive_stats_by_game.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY GAME AND LATENCY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descriptive_game_latency = df.groupby(['game_display', 'latency_ms']).agg({\n",
    "    'score': ['count', 'mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_game_latency)\n",
    "descriptive_game_latency.to_csv(OUTPUT_DIR / 'descriptive_stats_by_game_latency.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Examine correlations between latency and outcomes (performance, QoE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRELATION ANALYSIS: LATENCY vs OUTCOMES\n",
      "================================================================================\n",
      "\n",
      "OVERALL (All Games Combined):\n",
      "--------------------------------------------------------------------------------\n",
      "Latency vs Performance (z-score):\n",
      "  Pearson r = -0.454, p = 0.0000\n",
      "  Spearman ρ = -0.518, p = 0.0000\n",
      "\n",
      "Latency vs QoE Quality Rating:\n",
      "  Pearson r = -0.671, p = 0.0000\n",
      "  Spearman ρ = -0.673, p = 0.0000\n",
      "\n",
      "Latency vs Acceptability (binary):\n",
      "  Spearman ρ = 0.501, p = 0.0000\n",
      "\n",
      "Performance vs QoE Quality:\n",
      "  Pearson r = 0.382, p = 0.0000\n",
      "\n",
      "================================================================================\n",
      "PER-GAME CORRELATIONS\n",
      "================================================================================\n",
      "\n",
      "Dave the Diver:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.097 (p=0.4858), ρ=-0.354 (p=0.0086)\n",
      "  Latency vs QoE: r=-0.594 (p=0.0000)\n",
      "\n",
      "Feeding Frenzy:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.301 (p=0.0125), ρ=-0.298 (p=0.0135)\n",
      "  Latency vs QoE: r=-0.634 (p=0.0000)\n",
      "\n",
      "Fitts Law:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.918 (p=0.0000), ρ=-0.931 (p=0.0000)\n",
      "  Latency vs QoE: r=-0.874 (p=0.0000)\n",
      "\n",
      "Half-Life 2:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.541 (p=0.0000), ρ=-0.601 (p=0.0000)\n",
      "  Latency vs QoE: r=-0.742 (p=0.0000)\n",
      "\n",
      "Rocket League:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.342 (p=0.0151), ρ=-0.338 (p=0.0165)\n",
      "  Latency vs QoE: r=-0.604 (p=0.0000)\n",
      "\n",
      "Correlation results saved to: ../analysis/results/correlation_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS: LATENCY vs OUTCOMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correlation_results = []\n",
    "\n",
    "# Overall correlations\n",
    "print(\"\\nOVERALL (All Games Combined):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Latency vs Z-scored performance\n",
    "r_perf, p_perf = pearsonr(df['latency_ms'], df['score_z'])\n",
    "rho_perf, p_rho_perf = spearmanr(df['latency_ms'], df['score_z'])\n",
    "print(f\"Latency vs Performance (z-score):\")\n",
    "print(f\"  Pearson r = {r_perf:.3f}, p = {p_perf:.4f}\")\n",
    "print(f\"  Spearman ρ = {rho_perf:.3f}, p = {p_rho_perf:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'Performance (z)',\n",
    "    'pearson_r': r_perf,\n",
    "    'pearson_p': p_perf,\n",
    "    'spearman_rho': rho_perf,\n",
    "    'spearman_p': p_rho_perf,\n",
    "    'n': len(df)\n",
    "})\n",
    "\n",
    "# Latency vs QoE (quality rating)\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "r_qoe, p_qoe = pearsonr(qoe_valid['latency_ms'], qoe_valid['quality_rating'])\n",
    "rho_qoe, p_rho_qoe = spearmanr(qoe_valid['latency_ms'], qoe_valid['quality_rating'])\n",
    "print(f\"\\nLatency vs QoE Quality Rating:\")\n",
    "print(f\"  Pearson r = {r_qoe:.3f}, p = {p_qoe:.4f}\")\n",
    "print(f\"  Spearman ρ = {rho_qoe:.3f}, p = {p_rho_qoe:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'QoE Quality',\n",
    "    'pearson_r': r_qoe,\n",
    "    'pearson_p': p_qoe,\n",
    "    'spearman_rho': rho_qoe,\n",
    "    'spearman_p': p_rho_qoe,\n",
    "    'n': len(qoe_valid)\n",
    "})\n",
    "\n",
    "# Latency vs Acceptability\n",
    "accept_valid = df.dropna(subset=['acceptable'])\n",
    "rho_accept, p_rho_accept = spearmanr(accept_valid['latency_ms'], accept_valid['acceptable'])\n",
    "print(f\"\\nLatency vs Acceptability (binary):\")\n",
    "print(f\"  Spearman ρ = {rho_accept:.3f}, p = {p_rho_accept:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'Acceptability',\n",
    "    'pearson_r': np.nan,\n",
    "    'pearson_p': np.nan,\n",
    "    'spearman_rho': rho_accept,\n",
    "    'spearman_p': p_rho_accept,\n",
    "    'n': len(accept_valid)\n",
    "})\n",
    "\n",
    "# Performance vs QoE\n",
    "perf_qoe_valid = df.dropna(subset=['score_z', 'quality_rating'])\n",
    "r_perf_qoe, p_perf_qoe = pearsonr(perf_qoe_valid['score_z'], perf_qoe_valid['quality_rating'])\n",
    "print(f\"\\nPerformance vs QoE Quality:\")\n",
    "print(f\"  Pearson r = {r_perf_qoe:.3f}, p = {p_perf_qoe:.4f}\")\n",
    "\n",
    "# Per-game correlations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-GAME CORRELATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for game in sorted(df['game_display'].unique()):\n",
    "    game_df = df[df['game_display'] == game]\n",
    "\n",
    "    print(f\"\\n{game}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Latency vs Score (raw)\n",
    "    if len(game_df) > 2:\n",
    "        r_g, p_g = pearsonr(game_df['latency_ms'], game_df['score'])\n",
    "        rho_g, p_rho_g = spearmanr(game_df['latency_ms'], game_df['score'])\n",
    "        print(f\"  Latency vs Score: r={r_g:.3f} (p={p_g:.4f}), ρ={rho_g:.3f} (p={p_rho_g:.4f})\")\n",
    "\n",
    "        correlation_results.append({\n",
    "            'analysis': 'Per-game',\n",
    "            'game': game,\n",
    "            'outcome': 'Performance',\n",
    "            'pearson_r': r_g,\n",
    "            'pearson_p': p_g,\n",
    "            'spearman_rho': rho_g,\n",
    "            'spearman_p': p_rho_g,\n",
    "            'n': len(game_df)\n",
    "        })\n",
    "\n",
    "    # Latency vs QoE\n",
    "    game_qoe = game_df.dropna(subset=['quality_rating'])\n",
    "    if len(game_qoe) > 2:\n",
    "        r_qoe_g, p_qoe_g = pearsonr(game_qoe['latency_ms'], game_qoe['quality_rating'])\n",
    "        print(f\"  Latency vs QoE: r={r_qoe_g:.3f} (p={p_qoe_g:.4f})\")\n",
    "\n",
    "        correlation_results.append({\n",
    "            'analysis': 'Per-game',\n",
    "            'game': game,\n",
    "            'outcome': 'QoE Quality',\n",
    "            'pearson_r': r_qoe_g,\n",
    "            'pearson_p': p_qoe_g,\n",
    "            'spearman_rho': np.nan,\n",
    "            'spearman_p': np.nan,\n",
    "            'n': len(game_qoe)\n",
    "        })\n",
    "\n",
    "# Save correlation results\n",
    "corr_df = pd.DataFrame(correlation_results)\n",
    "corr_df.to_csv(OUTPUT_DIR / 'correlation_results.csv', index=False)\n",
    "print(f\"\\nCorrelation results saved to: {OUTPUT_DIR / 'correlation_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Repeated Measures ANOVA\n",
    "\n",
    "Test if latency significantly affects performance and QoE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REPEATED MEASURES ANOVA\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) by Latency - Overall:\n",
      "--------------------------------------------------------------------------------\n",
      "F-statistic: 26.662\n",
      "p-value: 0.0000\n",
      "\n",
      "QoE Quality Rating by Latency - Overall:\n",
      "--------------------------------------------------------------------------------\n",
      "F-statistic: 80.803\n",
      "p-value: 0.0000\n",
      "\n",
      "================================================================================\n",
      "PER-GAME ANOVA RESULTS\n",
      "================================================================================\n",
      "\n",
      "Dave the Diver:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=0.755, p=0.5249\n",
      "  QoE Quality: F=10.324, p=0.0000\n",
      "\n",
      "Feeding Frenzy:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=5.170, p=0.0029\n",
      "  QoE Quality: F=14.952, p=0.0000\n",
      "\n",
      "Fitts Law:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=129.173, p=0.0000\n",
      "  QoE Quality: F=99.690, p=0.0000\n",
      "\n",
      "Half-Life 2:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=8.416, p=0.0001\n",
      "  QoE Quality: F=26.564, p=0.0000\n",
      "\n",
      "Rocket League:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=2.447, p=0.0757\n",
      "  QoE Quality: F=9.932, p=0.0000\n",
      "\n",
      "ANOVA results saved to: ../analysis/results/anova_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"REPEATED MEASURES ANOVA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "# ANOVA for Performance (z-score) - Overall\n",
    "print(\"\\nPerformance (z-score) by Latency - Overall:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Simple one-way ANOVA (not accounting for repeated measures yet)\n",
    "latency_groups = [df[df['latency_ms'] == lat]['score_z'].dropna() for lat in sorted(df['latency_ms'].unique())]\n",
    "f_stat, p_val = f_oneway(*latency_groups)\n",
    "print(f\"F-statistic: {f_stat:.3f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "anova_results.append({\n",
    "    'outcome': 'Performance (z)',\n",
    "    'game': 'All',\n",
    "    'f_statistic': f_stat,\n",
    "    'p_value': p_val,\n",
    "    'n_groups': len(latency_groups),\n",
    "    'total_n': sum(len(g) for g in latency_groups)\n",
    "})\n",
    "\n",
    "# ANOVA for QoE Quality Rating\n",
    "print(\"\\nQoE Quality Rating by Latency - Overall:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "qoe_groups = [qoe_valid[qoe_valid['latency_ms'] == lat]['quality_rating'] for lat in sorted(qoe_valid['latency_ms'].unique())]\n",
    "f_stat_qoe, p_val_qoe = f_oneway(*qoe_groups)\n",
    "print(f\"F-statistic: {f_stat_qoe:.3f}\")\n",
    "print(f\"p-value: {p_val_qoe:.4f}\")\n",
    "\n",
    "anova_results.append({\n",
    "    'outcome': 'QoE Quality',\n",
    "    'game': 'All',\n",
    "    'f_statistic': f_stat_qoe,\n",
    "    'p_value': p_val_qoe,\n",
    "    'n_groups': len(qoe_groups),\n",
    "    'total_n': sum(len(g) for g in qoe_groups)\n",
    "})\n",
    "\n",
    "# Per-game ANOVAs\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-GAME ANOVA RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for game in sorted(df['game_display'].unique()):\n",
    "    game_df = df[df['game_display'] == game]\n",
    "\n",
    "    print(f\"\\n{game}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Performance ANOVA\n",
    "    game_groups = [game_df[game_df['latency_ms'] == lat]['score'].dropna() for lat in sorted(game_df['latency_ms'].unique())]\n",
    "    if all(len(g) > 0 for g in game_groups):\n",
    "        f_g, p_g = f_oneway(*game_groups)\n",
    "        print(f\"  Performance: F={f_g:.3f}, p={p_g:.4f}\")\n",
    "\n",
    "        anova_results.append({\n",
    "            'outcome': 'Performance',\n",
    "            'game': game,\n",
    "            'f_statistic': f_g,\n",
    "            'p_value': p_g,\n",
    "            'n_groups': len(game_groups),\n",
    "            'total_n': sum(len(g) for g in game_groups)\n",
    "        })\n",
    "\n",
    "    # QoE ANOVA\n",
    "    game_qoe = game_df.dropna(subset=['quality_rating'])\n",
    "    qoe_game_groups = [game_qoe[game_qoe['latency_ms'] == lat]['quality_rating'] for lat in sorted(game_qoe['latency_ms'].unique())]\n",
    "    if all(len(g) > 0 for g in qoe_game_groups):\n",
    "        f_qoe_g, p_qoe_g = f_oneway(*qoe_game_groups)\n",
    "        print(f\"  QoE Quality: F={f_qoe_g:.3f}, p={p_qoe_g:.4f}\")\n",
    "\n",
    "        anova_results.append({\n",
    "            'outcome': 'QoE Quality',\n",
    "            'game': game,\n",
    "            'f_statistic': f_qoe_g,\n",
    "            'p_value': p_qoe_g,\n",
    "            'n_groups': len(qoe_game_groups),\n",
    "            'total_n': sum(len(g) for g in qoe_game_groups)\n",
    "        })\n",
    "\n",
    "# Save ANOVA results\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df.to_csv(OUTPUT_DIR / 'anova_results.csv', index=False)\n",
    "print(f\"\\nANOVA results saved to: {OUTPUT_DIR / 'anova_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-hoc Pairwise Comparisons\n",
    "\n",
    "Tukey HSD test for pairwise latency comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "POST-HOC PAIRWISE COMPARISONS (Tukey HSD)\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) - All Games:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "   0.0   75.0  -0.2009 0.5044 -0.5734  0.1716  False\n",
      "   0.0  150.0  -0.6904    0.0 -1.0629 -0.3179   True\n",
      "   0.0  225.0  -1.1817    0.0 -1.5555  -0.808   True\n",
      "  75.0  150.0  -0.4895 0.0045 -0.8632 -0.1157   True\n",
      "  75.0  225.0  -0.9808    0.0 -1.3558 -0.6058   True\n",
      " 150.0  225.0  -0.4914 0.0045 -0.8663 -0.1164   True\n",
      "----------------------------------------------------\n",
      "\n",
      "QoE Quality Rating - All Games:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "   0.0   75.0  -0.7836    0.0 -1.2239 -0.3432   True\n",
      "   0.0  150.0  -1.7302    0.0 -2.1721 -1.2883   True\n",
      "   0.0  225.0  -2.4897    0.0 -2.9315 -2.0478   True\n",
      "  75.0  150.0  -0.9466    0.0   -1.39 -0.5033   True\n",
      "  75.0  225.0  -1.7061    0.0 -2.1494 -1.2628   True\n",
      " 150.0  225.0  -0.7595 0.0001 -1.2043 -0.3147   True\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"POST-HOC PAIRWISE COMPARISONS (Tukey HSD)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance (z-score)\n",
    "print(\"\\nPerformance (z-score) - All Games:\")\n",
    "print(\"-\" * 80)\n",
    "tukey_perf = pairwise_tukeyhsd(df['score_z'], df['latency_ms'])\n",
    "print(tukey_perf)\n",
    "\n",
    "# Save to file\n",
    "with open(OUTPUT_DIR / 'tukey_performance.txt', 'w') as f:\n",
    "    f.write(str(tukey_perf))\n",
    "\n",
    "# QoE Quality\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "print(\"\\nQoE Quality Rating - All Games:\")\n",
    "print(\"-\" * 80)\n",
    "tukey_qoe = pairwise_tukeyhsd(qoe_valid['quality_rating'], qoe_valid['latency_ms'])\n",
    "print(tukey_qoe)\n",
    "\n",
    "with open(OUTPUT_DIR / 'tukey_qoe.txt', 'w') as f:\n",
    "    f.write(str(tukey_qoe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect Sizes\n",
    "\n",
    "Calculate Cohen's d for pairwise latency comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EFFECT SIZES (Cohen's d)\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) - Baseline (0ms) vs Higher Latencies:\n",
      "--------------------------------------------------------------------------------\n",
      "0ms vs 75.0ms: d = 0.212\n",
      "0ms vs 150.0ms: d = 0.674\n",
      "0ms vs 225.0ms: d = 1.367\n",
      "\n",
      "QoE Quality - Baseline (0ms) vs Higher Latencies:\n",
      "--------------------------------------------------------------------------------\n",
      "0ms vs 75.0ms: d = 0.896\n",
      "0ms vs 150.0ms: d = 1.634\n",
      "0ms vs 225.0ms: d = 2.708\n",
      "\n",
      "Effect sizes saved to: ../analysis/results/effect_sizes.csv\n",
      "\n",
      "================================================================================\n",
      "Effect Size Interpretation (Cohen's d):\n",
      "  Small: d = 0.2\n",
      "  Medium: d = 0.5\n",
      "  Large: d = 0.8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EFFECT SIZES (Cohen's d)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "effect_sizes = []\n",
    "latencies = sorted(df['latency_ms'].unique())\n",
    "\n",
    "# Performance effect sizes\n",
    "print(\"\\nPerformance (z-score) - Baseline (0ms) vs Higher Latencies:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_perf = df[df['latency_ms'] == 0]['score_z'].dropna()\n",
    "\n",
    "for lat in latencies[1:]:\n",
    "    lat_perf = df[df['latency_ms'] == lat]['score_z'].dropna()\n",
    "    d = cohens_d(baseline_perf, lat_perf)\n",
    "    print(f\"0ms vs {lat}ms: d = {d:.3f}\")\n",
    "\n",
    "    effect_sizes.append({\n",
    "        'outcome': 'Performance (z)',\n",
    "        'comparison': f'0ms vs {lat}ms',\n",
    "        'cohens_d': d,\n",
    "        'n_baseline': len(baseline_perf),\n",
    "        'n_comparison': len(lat_perf)\n",
    "    })\n",
    "\n",
    "# QoE effect sizes\n",
    "print(\"\\nQoE Quality - Baseline (0ms) vs Higher Latencies:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_qoe = df[df['latency_ms'] == 0]['quality_rating'].dropna()\n",
    "\n",
    "for lat in latencies[1:]:\n",
    "    lat_qoe = df[df['latency_ms'] == lat]['quality_rating'].dropna()\n",
    "    d_qoe = cohens_d(baseline_qoe, lat_qoe)\n",
    "    print(f\"0ms vs {lat}ms: d = {d_qoe:.3f}\")\n",
    "\n",
    "    effect_sizes.append({\n",
    "        'outcome': 'QoE Quality',\n",
    "        'comparison': f'0ms vs {lat}ms',\n",
    "        'cohens_d': d_qoe,\n",
    "        'n_baseline': len(baseline_qoe),\n",
    "        'n_comparison': len(lat_qoe)\n",
    "    })\n",
    "\n",
    "# Save effect sizes\n",
    "effect_df = pd.DataFrame(effect_sizes)\n",
    "effect_df.to_csv(OUTPUT_DIR / 'effect_sizes.csv', index=False)\n",
    "print(f\"\\nEffect sizes saved to: {OUTPUT_DIR / 'effect_sizes.csv'}\")\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Effect Size Interpretation (Cohen's d):\")\n",
    "print(\"  Small: d = 0.2\")\n",
    "print(\"  Medium: d = 0.5\")\n",
    "print(\"  Large: d = 0.8\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear Mixed-Effects Models\n",
    "\n",
    "Properly account for within-subjects design with random effects for\n",
    "participants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LINEAR MIXED-EFFECTS MODELS\n",
      "================================================================================\n",
      "\n",
      "Model 1: Performance ~ Latency + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n",
      "         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: score_z  \n",
      "No. Observations: 300     Method:             REML     \n",
      "No. Groups:       17      Scale:              0.7621   \n",
      "Min. group size:  8       Log-Likelihood:     -396.1750\n",
      "Max. group size:  20      Converged:          Yes      \n",
      "Mean group size:  17.6                                 \n",
      "-------------------------------------------------------\n",
      "             Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept     0.596    0.093  6.429 0.000  0.414  0.778\n",
      "latency_ms   -0.005    0.001 -8.942 0.000 -0.007 -0.004\n",
      "Group Var     0.025    0.030                           \n",
      "=======================================================\n",
      "\n",
      "\n",
      "Model 2: QoE Quality ~ Latency + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n",
      "           Mixed Linear Model Regression Results\n",
      "============================================================\n",
      "Model:            MixedLM Dependent Variable: quality_rating\n",
      "No. Observations: 299     Method:             REML          \n",
      "No. Groups:       17      Scale:              0.9640        \n",
      "Min. group size:  8       Log-Likelihood:     -435.7392     \n",
      "Max. group size:  20      Converged:          Yes           \n",
      "Mean group size:  17.6                                      \n",
      "-------------------------------------------------------------\n",
      "             Coef.   Std.Err.     z     P>|z|  [0.025  0.975]\n",
      "-------------------------------------------------------------\n",
      "Intercept     4.561     0.129   35.319  0.000   4.308   4.814\n",
      "latency_ms   -0.011     0.001  -16.593  0.000  -0.013  -0.010\n",
      "Group Var     0.130     0.068                                \n",
      "============================================================\n",
      "\n",
      "\n",
      "Model 3: QoE Quality ~ Latency + Performance + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n",
      "           Mixed Linear Model Regression Results\n",
      "============================================================\n",
      "Model:            MixedLM Dependent Variable: quality_rating\n",
      "No. Observations: 299     Method:             REML          \n",
      "No. Groups:       17      Scale:              0.9554        \n",
      "Min. group size:  8       Log-Likelihood:     -435.6494     \n",
      "Max. group size:  20      Converged:          Yes           \n",
      "Mean group size:  17.6                                      \n",
      "-------------------------------------------------------------\n",
      "             Coef.   Std.Err.     z     P>|z|  [0.025  0.975]\n",
      "-------------------------------------------------------------\n",
      "Intercept     4.485     0.134   33.453  0.000   4.222   4.748\n",
      "latency_ms   -0.011     0.001  -13.847  0.000  -0.012  -0.009\n",
      "score_z       0.128     0.066    1.948  0.051  -0.001   0.257\n",
      "Group Var     0.127     0.067                                \n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LINEAR MIXED-EFFECTS MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Model 1: Performance (z-score) predicted by latency\n",
    "print(\"\\nModel 1: Performance ~ Latency + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "model1 = mixedlm(\n",
    "    \"score_z ~ latency_ms\",\n",
    "    data=df,\n",
    "    groups=df[\"participant_id\"]\n",
    ")\n",
    "result1 = model1.fit()\n",
    "print(result1.summary())\n",
    "\n",
    "# Save model summary\n",
    "with open(OUTPUT_DIR / 'mixed_model_performance.txt', 'w') as f:\n",
    "    f.write(str(result1.summary()))\n",
    "\n",
    "# Model 2: QoE Quality predicted by latency\n",
    "print(\"\\nModel 2: QoE Quality ~ Latency + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "model2 = mixedlm(\n",
    "    \"quality_rating ~ latency_ms\",\n",
    "    data=qoe_valid,\n",
    "    groups=qoe_valid[\"participant_id\"]\n",
    ")\n",
    "result2 = model2.fit()\n",
    "print(result2.summary())\n",
    "\n",
    "with open(OUTPUT_DIR / 'mixed_model_qoe.txt', 'w') as f:\n",
    "    f.write(str(result2.summary()))\n",
    "\n",
    "# Model 3: QoE Quality predicted by latency AND performance\n",
    "print(\"\\nModel 3: QoE Quality ~ Latency + Performance + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "perf_qoe_valid = df.dropna(subset=['quality_rating', 'score_z'])\n",
    "model3 = mixedlm(\n",
    "    \"quality_rating ~ latency_ms + score_z\",\n",
    "    data=perf_qoe_valid,\n",
    "    groups=perf_qoe_valid[\"participant_id\"]\n",
    ")\n",
    "result3 = model3.fit()\n",
    "print(result3.summary())\n",
    "\n",
    "with open(OUTPUT_DIR / 'mixed_model_qoe_with_performance.txt', 'w') as f:\n",
    "    f.write(str(result3.summary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table\n",
    "\n",
    "Create a publication-ready summary table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY TABLE FOR PUBLICATION\n",
      "================================================================================\n",
      "            Performance Mean  Performance SD  QoE Mean  QoE SD  \\\n",
      "latency_ms                                                       \n",
      "0.0                    0.514           1.059     4.533   0.738   \n",
      "75.0                   0.313           0.819     3.749   0.995   \n",
      "150.0                 -0.176           0.987     2.803   1.309   \n",
      "225.0                 -0.667           0.602     2.043   1.075   \n",
      "\n",
      "            Acceptability %   N  \n",
      "latency_ms                       \n",
      "0.0                   103.9  76  \n",
      "75.0                  120.0  75  \n",
      "150.0                 144.6  74  \n",
      "225.0                 166.2  74  \n",
      "\n",
      "Summary table saved to:\n",
      "  - ../analysis/results/summary_table.csv\n",
      "  - ../analysis/results/summary_table.tex\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY TABLE FOR PUBLICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_table = df.groupby('latency_ms').agg({\n",
    "    'score_z': ['mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean', 'count']\n",
    "}).round(3)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_table.columns = [\n",
    "    'Performance Mean', 'Performance SD',\n",
    "    'QoE Mean', 'QoE SD',\n",
    "    'Acceptability %', 'N'\n",
    "]\n",
    "\n",
    "# Convert acceptability to percentage\n",
    "summary_table['Acceptability %'] = (summary_table['Acceptability %'] * 100).round(1)\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "# Save to CSV and LaTeX format\n",
    "summary_table.to_csv(OUTPUT_DIR / 'summary_table.csv')\n",
    "summary_table.to_latex(OUTPUT_DIR / 'summary_table.tex')\n",
    "\n",
    "print(f\"\\nSummary table saved to:\")\n",
    "print(f\"  - {OUTPUT_DIR / 'summary_table.csv'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'summary_table.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Statistical analysis complete! Key findings:\n",
    "\n",
    "1. **Correlation Analysis**: Examined relationships between latency and outcomes\n",
    "2. **ANOVA**: Tested significance of latency effects\n",
    "3. **Post-hoc Tests**: Identified which latency pairs differ significantly\n",
    "4. **Effect Sizes**: Quantified practical significance of latency impacts\n",
    "5. **Mixed Models**: Properly accounted for within-subjects design\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- Run `visualizations.ipynb` to create figures\n",
    "- Review all output files in `analysis/results/`\n",
    "- Interpret findings in the context of your research questions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
