{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPSci Latency Study - Statistical Analysis\n",
    "\n",
    "This notebook performs statistical analyses on the processed experimental data.\n",
    "\n",
    "## Analyses\n",
    "\n",
    "1. Descriptive statistics\n",
    "2. Correlation analysis (latency vs performance, latency vs QoE)\n",
    "3. Mixed-effects models (within-subjects repeated measures)\n",
    "4. Effect sizes and pairwise comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols, mixedlm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 488 observations\n",
      "Participants: 27\n",
      "Games: <StringArray>\n",
      "['Dave the Diver', 'Half-Life 2', 'Fitts Law', 'Feeding Frenzy',\n",
      " 'Rocket League']\n",
      "Length: 5, dtype: str\n",
      "Latency conditions: [np.int64(0), np.int64(75), np.int64(150), np.int64(225)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>game</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>score</th>\n",
       "      <th>mean_time_ms</th>\n",
       "      <th>median_time_ms</th>\n",
       "      <th>throughput</th>\n",
       "      <th>n_trials</th>\n",
       "      <th>goals</th>\n",
       "      <th>saves</th>\n",
       "      <th>assists</th>\n",
       "      <th>shots</th>\n",
       "      <th>quality_rating</th>\n",
       "      <th>acceptable</th>\n",
       "      <th>game_display</th>\n",
       "      <th>score_z</th>\n",
       "      <th>score_pct_of_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>150</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>0.660204</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>0.404248</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>0.233611</td>\n",
       "      <td>83.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>dave_the_diver</td>\n",
       "      <td>225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dave the Diver</td>\n",
       "      <td>-1.046169</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>half_life_2</td>\n",
       "      <td>75</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Half-Life 2</td>\n",
       "      <td>0.826965</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id            game  latency_ms  score  mean_time_ms  \\\n",
       "0               1  dave_the_diver         150    3.0           NaN   \n",
       "1               1  dave_the_diver           0    2.7           NaN   \n",
       "2               1  dave_the_diver          75    2.5           NaN   \n",
       "3               1  dave_the_diver         225    1.0           NaN   \n",
       "4               1     half_life_2          75   15.0           NaN   \n",
       "\n",
       "   median_time_ms  throughput  n_trials  goals  saves  assists  shots  \\\n",
       "0             NaN         NaN       NaN    NaN    NaN      NaN    NaN   \n",
       "1             NaN         NaN       NaN    NaN    NaN      NaN    NaN   \n",
       "2             NaN         NaN       NaN    NaN    NaN      NaN    NaN   \n",
       "3             NaN         NaN       NaN    NaN    NaN      NaN    NaN   \n",
       "4             NaN         NaN       NaN    NaN    NaN      NaN    NaN   \n",
       "\n",
       "   quality_rating  acceptable    game_display   score_z  score_pct_of_baseline  \n",
       "0             4.0         1.0  Dave the Diver  0.660204             100.000000  \n",
       "1             5.0         1.0  Dave the Diver  0.404248              90.000000  \n",
       "2             5.0         1.0  Dave the Diver  0.233611              83.333333  \n",
       "3             3.2         0.0  Dave the Diver -1.046169              33.333333  \n",
       "4             4.5         1.0     Half-Life 2  0.826965             100.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('../analysis/processed_data')\n",
    "OUTPUT_DIR = Path('../analysis/results')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load combined data\n",
    "df = pd.read_csv(DATA_DIR / 'combined_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} observations\")\n",
    "print(f\"Participants: {len(df['participant_id'].unique())}\")\n",
    "print(f\"Games: {df['game_display'].unique()}\")\n",
    "print(f\"Latency conditions: {sorted(df['latency_ms'].unique())}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY LATENCY CONDITION\n",
      "================================================================================\n",
      "           score                                score_z        quality_rating  \\\n",
      "           count     mean      std  min     max    mean    std           mean   \n",
      "latency_ms                                                                      \n",
      "0            122  223.497  369.370  0.1  1490.0   0.750  0.970          4.485   \n",
      "75           122  166.476  273.102  0.2  1150.0   0.279  0.800          3.793   \n",
      "150          122  115.173  216.991  0.0  1600.0  -0.309  0.786          3.026   \n",
      "225          122   68.931   98.342  0.0   420.0  -0.720  0.726          2.321   \n",
      "\n",
      "                  acceptable         \n",
      "              std       mean    sum  \n",
      "latency_ms                           \n",
      "0           0.775      0.967  118.0  \n",
      "75          0.962      0.836  102.0  \n",
      "150         1.297      0.595   72.0  \n",
      "225         1.155      0.402   49.0  \n",
      "\n",
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY GAME\n",
      "================================================================================\n",
      "               score                                    quality_rating         \\\n",
      "               count     mean      std    min       max           mean    std   \n",
      "game_display                                                                    \n",
      "Dave the Diver    84    2.226    1.172   0.00     4.500          3.906  1.095   \n",
      "Feeding Frenzy   108  493.333  359.101  70.00  1600.000          3.588  1.150   \n",
      "Fitts Law        104    3.408    1.182   1.65     5.993          2.799  1.446   \n",
      "Half-Life 2      104   12.240    3.337   2.00    21.000          3.145  1.443   \n",
      "Rocket League     88  169.807  128.435   0.00   527.000          3.743  1.171   \n",
      "\n",
      "               acceptable  \n",
      "                     mean  \n",
      "game_display               \n",
      "Dave the Diver      0.867  \n",
      "Feeding Frenzy      0.796  \n",
      "Fitts Law           0.490  \n",
      "Half-Life 2         0.587  \n",
      "Rocket League       0.807  \n",
      "\n",
      "================================================================================\n",
      "DESCRIPTIVE STATISTICS BY GAME AND LATENCY\n",
      "================================================================================\n",
      "                          score                   quality_rating         \\\n",
      "                          count     mean      std           mean    std   \n",
      "game_display   latency_ms                                                 \n",
      "Dave the Diver 0             21    2.538    1.129          4.548  0.669   \n",
      "               75            21    2.610    1.149          4.238  0.785   \n",
      "               150           21    2.095    1.028          3.850  1.001   \n",
      "               225           21    1.662    1.200          2.986  1.210   \n",
      "Feeding Frenzy 0             27  818.519  345.985          4.314  0.979   \n",
      "               75            27  566.667  306.419          3.874  0.962   \n",
      "               150           27  386.667  310.521          3.537  1.080   \n",
      "               225           27  201.481   70.749          2.626  0.890   \n",
      "Fitts Law      0             26    5.013    0.568          4.800  0.400   \n",
      "               75            26    3.742    0.509          2.950  0.788   \n",
      "               150           26    2.696    0.449          1.946  0.787   \n",
      "               225           26    2.181    0.286          1.500  0.721   \n",
      "Half-Life 2    0             26   14.731    2.961          4.504  0.788   \n",
      "               75            26   13.308    2.346          3.819  0.904   \n",
      "               150           26   11.269    2.554          2.523  1.185   \n",
      "               225           26    9.654    3.085          1.735  0.938   \n",
      "Rocket League  0             22  209.091  143.505          4.241  0.830   \n",
      "               75            22  205.091  131.663          4.232  0.772   \n",
      "               150           22  145.636  119.807          3.523  1.362   \n",
      "               225           22  119.409   99.312          2.977  1.170   \n",
      "\n",
      "                          acceptable  \n",
      "                                mean  \n",
      "game_display   latency_ms             \n",
      "Dave the Diver 0               1.000  \n",
      "               75              0.952  \n",
      "               150             0.850  \n",
      "               225             0.667  \n",
      "Feeding Frenzy 0               0.963  \n",
      "               75              0.926  \n",
      "               150             0.852  \n",
      "               225             0.444  \n",
      "Fitts Law      0               1.000  \n",
      "               75              0.538  \n",
      "               150             0.231  \n",
      "               225             0.192  \n",
      "Half-Life 2    0               0.962  \n",
      "               75              0.846  \n",
      "               150             0.346  \n",
      "               225             0.192  \n",
      "Rocket League  0               0.909  \n",
      "               75              0.955  \n",
      "               150             0.773  \n",
      "               225             0.591  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY LATENCY CONDITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Overall statistics by latency\n",
    "descriptive_latency = df.groupby('latency_ms').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'score_z': ['mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean', 'sum']\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_latency)\n",
    "\n",
    "# Save to file\n",
    "descriptive_latency.to_csv(OUTPUT_DIR / 'descriptive_stats_by_latency.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY GAME\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descriptive_game = df.groupby('game_display').agg({\n",
    "    'score': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean']\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_game)\n",
    "descriptive_game.to_csv(OUTPUT_DIR / 'descriptive_stats_by_game.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS BY GAME AND LATENCY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descriptive_game_latency = df.groupby(['game_display', 'latency_ms']).agg({\n",
    "    'score': ['count', 'mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(descriptive_game_latency)\n",
    "descriptive_game_latency.to_csv(OUTPUT_DIR / 'descriptive_stats_by_game_latency.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis\n",
    "\n",
    "Examine correlations between latency and outcomes (performance, QoE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRELATION ANALYSIS: LATENCY vs OUTCOMES\n",
      "================================================================================\n",
      "\n",
      "OVERALL (All Games Combined):\n",
      "--------------------------------------------------------------------------------\n",
      "Latency vs Performance (z-score):\n",
      "  Pearson r = -0.562, p = 0.0000\n",
      "  Spearman ρ = -0.576, p = 0.0000\n",
      "\n",
      "Latency vs QoE Quality Rating:\n",
      "  Pearson r = -0.608, p = 0.0000\n",
      "  Spearman ρ = -0.609, p = 0.0000\n",
      "\n",
      "Latency vs Acceptability (binary):\n",
      "  Spearman ρ = -0.473, p = 0.0000\n",
      "\n",
      "Performance vs QoE Quality:\n",
      "  Pearson r = 0.433, p = 0.0000\n",
      "\n",
      "================================================================================\n",
      "PER-GAME CORRELATIONS\n",
      "================================================================================\n",
      "\n",
      "Dave the Diver:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.302 (p=0.0053), ρ=-0.301 (p=0.0053)\n",
      "  Latency vs QoE: r=-0.524 (p=0.0000)\n",
      "\n",
      "Feeding Frenzy:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.635 (p=0.0000), ρ=-0.701 (p=0.0000)\n",
      "  Latency vs QoE: r=-0.528 (p=0.0000)\n",
      "\n",
      "Fitts Law:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.907 (p=0.0000), ρ=-0.914 (p=0.0000)\n",
      "  Latency vs QoE: r=-0.847 (p=0.0000)\n",
      "\n",
      "Half-Life 2:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.581 (p=0.0000), ρ=-0.612 (p=0.0000)\n",
      "  Latency vs QoE: r=-0.747 (p=0.0000)\n",
      "\n",
      "Rocket League:\n",
      "--------------------------------------------------------------------------------\n",
      "  Latency vs Score: r=-0.288 (p=0.0066), ρ=-0.290 (p=0.0061)\n",
      "  Latency vs QoE: r=-0.432 (p=0.0000)\n",
      "\n",
      "Correlation results saved to: ../analysis/results/correlation_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS: LATENCY vs OUTCOMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correlation_results = []\n",
    "\n",
    "# Overall correlations\n",
    "print(\"\\nOVERALL (All Games Combined):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Latency vs Z-scored performance\n",
    "r_perf, p_perf = pearsonr(df['latency_ms'], df['score_z'])\n",
    "rho_perf, p_rho_perf = spearmanr(df['latency_ms'], df['score_z'])\n",
    "print(f\"Latency vs Performance (z-score):\")\n",
    "print(f\"  Pearson r = {r_perf:.3f}, p = {p_perf:.4f}\")\n",
    "print(f\"  Spearman ρ = {rho_perf:.3f}, p = {p_rho_perf:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'Performance (z)',\n",
    "    'pearson_r': r_perf,\n",
    "    'pearson_p': p_perf,\n",
    "    'spearman_rho': rho_perf,\n",
    "    'spearman_p': p_rho_perf,\n",
    "    'n': len(df)\n",
    "})\n",
    "\n",
    "# Latency vs QoE (quality rating)\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "r_qoe, p_qoe = pearsonr(qoe_valid['latency_ms'], qoe_valid['quality_rating'])\n",
    "rho_qoe, p_rho_qoe = spearmanr(qoe_valid['latency_ms'], qoe_valid['quality_rating'])\n",
    "print(f\"\\nLatency vs QoE Quality Rating:\")\n",
    "print(f\"  Pearson r = {r_qoe:.3f}, p = {p_qoe:.4f}\")\n",
    "print(f\"  Spearman ρ = {rho_qoe:.3f}, p = {p_rho_qoe:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'QoE Quality',\n",
    "    'pearson_r': r_qoe,\n",
    "    'pearson_p': p_qoe,\n",
    "    'spearman_rho': rho_qoe,\n",
    "    'spearman_p': p_rho_qoe,\n",
    "    'n': len(qoe_valid)\n",
    "})\n",
    "\n",
    "# Latency vs Acceptability\n",
    "accept_valid = df.dropna(subset=['acceptable'])\n",
    "rho_accept, p_rho_accept = spearmanr(accept_valid['latency_ms'], accept_valid['acceptable'])\n",
    "print(f\"\\nLatency vs Acceptability (binary):\")\n",
    "print(f\"  Spearman ρ = {rho_accept:.3f}, p = {p_rho_accept:.4f}\")\n",
    "\n",
    "correlation_results.append({\n",
    "    'analysis': 'Overall',\n",
    "    'game': 'All',\n",
    "    'outcome': 'Acceptability',\n",
    "    'pearson_r': np.nan,\n",
    "    'pearson_p': np.nan,\n",
    "    'spearman_rho': rho_accept,\n",
    "    'spearman_p': p_rho_accept,\n",
    "    'n': len(accept_valid)\n",
    "})\n",
    "\n",
    "# Performance vs QoE\n",
    "perf_qoe_valid = df.dropna(subset=['score_z', 'quality_rating'])\n",
    "r_perf_qoe, p_perf_qoe = pearsonr(perf_qoe_valid['score_z'], perf_qoe_valid['quality_rating'])\n",
    "print(f\"\\nPerformance vs QoE Quality:\")\n",
    "print(f\"  Pearson r = {r_perf_qoe:.3f}, p = {p_perf_qoe:.4f}\")\n",
    "\n",
    "# Per-game correlations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-GAME CORRELATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for game in sorted(df['game_display'].unique()):\n",
    "    game_df = df[df['game_display'] == game]\n",
    "\n",
    "    print(f\"\\n{game}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Latency vs Score (raw)\n",
    "    if len(game_df) > 2:\n",
    "        r_g, p_g = pearsonr(game_df['latency_ms'], game_df['score'])\n",
    "        rho_g, p_rho_g = spearmanr(game_df['latency_ms'], game_df['score'])\n",
    "        print(f\"  Latency vs Score: r={r_g:.3f} (p={p_g:.4f}), ρ={rho_g:.3f} (p={p_rho_g:.4f})\")\n",
    "\n",
    "        correlation_results.append({\n",
    "            'analysis': 'Per-game',\n",
    "            'game': game,\n",
    "            'outcome': 'Performance',\n",
    "            'pearson_r': r_g,\n",
    "            'pearson_p': p_g,\n",
    "            'spearman_rho': rho_g,\n",
    "            'spearman_p': p_rho_g,\n",
    "            'n': len(game_df)\n",
    "        })\n",
    "\n",
    "    # Latency vs QoE\n",
    "    game_qoe = game_df.dropna(subset=['quality_rating'])\n",
    "    if len(game_qoe) > 2:\n",
    "        r_qoe_g, p_qoe_g = pearsonr(game_qoe['latency_ms'], game_qoe['quality_rating'])\n",
    "        print(f\"  Latency vs QoE: r={r_qoe_g:.3f} (p={p_qoe_g:.4f})\")\n",
    "\n",
    "        correlation_results.append({\n",
    "            'analysis': 'Per-game',\n",
    "            'game': game,\n",
    "            'outcome': 'QoE Quality',\n",
    "            'pearson_r': r_qoe_g,\n",
    "            'pearson_p': p_qoe_g,\n",
    "            'spearman_rho': np.nan,\n",
    "            'spearman_p': np.nan,\n",
    "            'n': len(game_qoe)\n",
    "        })\n",
    "\n",
    "# Save correlation results\n",
    "corr_df = pd.DataFrame(correlation_results)\n",
    "corr_df.to_csv(OUTPUT_DIR / 'correlation_results.csv', index=False)\n",
    "print(f\"\\nCorrelation results saved to: {OUTPUT_DIR / 'correlation_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Repeated Measures ANOVA\n",
    "\n",
    "Test if latency significantly affects performance and QoE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REPEATED MEASURES ANOVA\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) by Latency - Overall:\n",
      "--------------------------------------------------------------------------------\n",
      "F-statistic: 74.833\n",
      "p-value: 0.0000\n",
      "\n",
      "QoE Quality Rating by Latency - Overall:\n",
      "--------------------------------------------------------------------------------\n",
      "F-statistic: 94.410\n",
      "p-value: 0.0000\n",
      "\n",
      "================================================================================\n",
      "PER-GAME ANOVA RESULTS\n",
      "================================================================================\n",
      "\n",
      "Dave the Diver:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=3.188, p=0.0281\n",
      "  QoE Quality: F=10.900, p=0.0000\n",
      "\n",
      "Feeding Frenzy:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=23.733, p=0.0000\n",
      "  QoE Quality: F=14.389, p=0.0000\n",
      "\n",
      "Fitts Law:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=188.465, p=0.0000\n",
      "  QoE Quality: F=116.282, p=0.0000\n",
      "\n",
      "Half-Life 2:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=17.129, p=0.0000\n",
      "  QoE Quality: F=43.578, p=0.0000\n",
      "\n",
      "Rocket League:\n",
      "--------------------------------------------------------------------------------\n",
      "  Performance: F=2.791, p=0.0454\n",
      "  QoE Quality: F=7.295, p=0.0002\n",
      "\n",
      "ANOVA results saved to: ../analysis/results/anova_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"REPEATED MEASURES ANOVA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "# ANOVA for Performance (z-score) - Overall\n",
    "print(\"\\nPerformance (z-score) by Latency - Overall:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Simple one-way ANOVA (not accounting for repeated measures yet)\n",
    "latency_groups = [df[df['latency_ms'] == lat]['score_z'].dropna() for lat in sorted(df['latency_ms'].unique())]\n",
    "f_stat, p_val = f_oneway(*latency_groups)\n",
    "print(f\"F-statistic: {f_stat:.3f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "\n",
    "anova_results.append({\n",
    "    'outcome': 'Performance (z)',\n",
    "    'game': 'All',\n",
    "    'f_statistic': f_stat,\n",
    "    'p_value': p_val,\n",
    "    'n_groups': len(latency_groups),\n",
    "    'total_n': sum(len(g) for g in latency_groups)\n",
    "})\n",
    "\n",
    "# ANOVA for QoE Quality Rating\n",
    "print(\"\\nQoE Quality Rating by Latency - Overall:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "qoe_groups = [qoe_valid[qoe_valid['latency_ms'] == lat]['quality_rating'] for lat in sorted(qoe_valid['latency_ms'].unique())]\n",
    "f_stat_qoe, p_val_qoe = f_oneway(*qoe_groups)\n",
    "print(f\"F-statistic: {f_stat_qoe:.3f}\")\n",
    "print(f\"p-value: {p_val_qoe:.4f}\")\n",
    "\n",
    "anova_results.append({\n",
    "    'outcome': 'QoE Quality',\n",
    "    'game': 'All',\n",
    "    'f_statistic': f_stat_qoe,\n",
    "    'p_value': p_val_qoe,\n",
    "    'n_groups': len(qoe_groups),\n",
    "    'total_n': sum(len(g) for g in qoe_groups)\n",
    "})\n",
    "\n",
    "# Per-game ANOVAs\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-GAME ANOVA RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for game in sorted(df['game_display'].unique()):\n",
    "    game_df = df[df['game_display'] == game]\n",
    "\n",
    "    print(f\"\\n{game}:\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Performance ANOVA\n",
    "    game_groups = [game_df[game_df['latency_ms'] == lat]['score'].dropna() for lat in sorted(game_df['latency_ms'].unique())]\n",
    "    if all(len(g) > 0 for g in game_groups):\n",
    "        f_g, p_g = f_oneway(*game_groups)\n",
    "        print(f\"  Performance: F={f_g:.3f}, p={p_g:.4f}\")\n",
    "\n",
    "        anova_results.append({\n",
    "            'outcome': 'Performance',\n",
    "            'game': game,\n",
    "            'f_statistic': f_g,\n",
    "            'p_value': p_g,\n",
    "            'n_groups': len(game_groups),\n",
    "            'total_n': sum(len(g) for g in game_groups)\n",
    "        })\n",
    "\n",
    "    # QoE ANOVA\n",
    "    game_qoe = game_df.dropna(subset=['quality_rating'])\n",
    "    qoe_game_groups = [game_qoe[game_qoe['latency_ms'] == lat]['quality_rating'] for lat in sorted(game_qoe['latency_ms'].unique())]\n",
    "    if all(len(g) > 0 for g in qoe_game_groups):\n",
    "        f_qoe_g, p_qoe_g = f_oneway(*qoe_game_groups)\n",
    "        print(f\"  QoE Quality: F={f_qoe_g:.3f}, p={p_qoe_g:.4f}\")\n",
    "\n",
    "        anova_results.append({\n",
    "            'outcome': 'QoE Quality',\n",
    "            'game': game,\n",
    "            'f_statistic': f_qoe_g,\n",
    "            'p_value': p_qoe_g,\n",
    "            'n_groups': len(qoe_game_groups),\n",
    "            'total_n': sum(len(g) for g in qoe_game_groups)\n",
    "        })\n",
    "\n",
    "# Save ANOVA results\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df.to_csv(OUTPUT_DIR / 'anova_results.csv', index=False)\n",
    "print(f\"\\nANOVA results saved to: {OUTPUT_DIR / 'anova_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-hoc Pairwise Comparisons\n",
    "\n",
    "Tukey HSD test for pairwise latency comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "POST-HOC PAIRWISE COMPARISONS (Tukey HSD)\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) - All Games:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "     0     75  -0.4705 0.0001  -0.743  -0.198   True\n",
      "     0    150  -1.0592    0.0 -1.3318 -0.7867   True\n",
      "     0    225  -1.4699    0.0 -1.7424 -1.1973   True\n",
      "    75    150  -0.5887    0.0 -0.8613 -0.3162   True\n",
      "    75    225  -0.9994    0.0 -1.2719 -0.7269   True\n",
      "   150    225  -0.4107 0.0007 -0.6832 -0.1381   True\n",
      "----------------------------------------------------\n",
      "\n",
      "QoE Quality Rating - All Games:\n",
      "--------------------------------------------------------------------------------\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      "group1 group2 meandiff p-adj  lower   upper  reject\n",
      "---------------------------------------------------\n",
      "     0     75  -0.6924   0.0 -1.0439 -0.3408   True\n",
      "     0    150  -1.4586   0.0 -1.8108 -1.1063   True\n",
      "     0    225  -2.1637   0.0 -2.5152 -1.8122   True\n",
      "    75    150  -0.7662   0.0 -1.1184 -0.4139   True\n",
      "    75    225  -1.4713   0.0 -1.8228 -1.1198   True\n",
      "   150    225  -0.7051   0.0 -1.0574 -0.3529   True\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"POST-HOC PAIRWISE COMPARISONS (Tukey HSD)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Performance (z-score)\n",
    "print(\"\\nPerformance (z-score) - All Games:\")\n",
    "print(\"-\" * 80)\n",
    "tukey_perf = pairwise_tukeyhsd(df['score_z'], df['latency_ms'])\n",
    "print(tukey_perf)\n",
    "\n",
    "# Save to file\n",
    "with open(OUTPUT_DIR / 'tukey_performance.txt', 'w') as f:\n",
    "    f.write(str(tukey_perf))\n",
    "\n",
    "# QoE Quality\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "print(\"\\nQoE Quality Rating - All Games:\")\n",
    "print(\"-\" * 80)\n",
    "tukey_qoe = pairwise_tukeyhsd(qoe_valid['quality_rating'], qoe_valid['latency_ms'])\n",
    "print(tukey_qoe)\n",
    "\n",
    "with open(OUTPUT_DIR / 'tukey_qoe.txt', 'w') as f:\n",
    "    f.write(str(tukey_qoe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effect Sizes\n",
    "\n",
    "Calculate Cohen's d for pairwise latency comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EFFECT SIZES (Cohen's d)\n",
      "================================================================================\n",
      "\n",
      "Performance (z-score) - Baseline (0ms) vs Higher Latencies:\n",
      "--------------------------------------------------------------------------------\n",
      "0ms vs 75ms: d = 0.529\n",
      "0ms vs 150ms: d = 1.200\n",
      "0ms vs 225ms: d = 1.715\n",
      "\n",
      "QoE Quality - Baseline (0ms) vs Higher Latencies:\n",
      "--------------------------------------------------------------------------------\n",
      "0ms vs 75ms: d = 0.793\n",
      "0ms vs 150ms: d = 1.367\n",
      "0ms vs 225ms: d = 2.200\n",
      "\n",
      "Effect sizes saved to: ../analysis/results/effect_sizes.csv\n",
      "\n",
      "================================================================================\n",
      "Effect Size Interpretation (Cohen's d):\n",
      "  Small: d = 0.2\n",
      "  Medium: d = 0.5\n",
      "  Large: d = 0.8\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EFFECT SIZES (Cohen's d)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "effect_sizes = []\n",
    "latencies = sorted(df['latency_ms'].unique())\n",
    "\n",
    "# Performance effect sizes\n",
    "print(\"\\nPerformance (z-score) - Baseline (0ms) vs Higher Latencies:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_perf = df[df['latency_ms'] == 0]['score_z'].dropna()\n",
    "\n",
    "for lat in latencies[1:]:\n",
    "    lat_perf = df[df['latency_ms'] == lat]['score_z'].dropna()\n",
    "    d = cohens_d(baseline_perf, lat_perf)\n",
    "    print(f\"0ms vs {lat}ms: d = {d:.3f}\")\n",
    "\n",
    "    effect_sizes.append({\n",
    "        'outcome': 'Performance (z)',\n",
    "        'comparison': f'0ms vs {lat}ms',\n",
    "        'cohens_d': d,\n",
    "        'n_baseline': len(baseline_perf),\n",
    "        'n_comparison': len(lat_perf)\n",
    "    })\n",
    "\n",
    "# QoE effect sizes\n",
    "print(\"\\nQoE Quality - Baseline (0ms) vs Higher Latencies:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_qoe = df[df['latency_ms'] == 0]['quality_rating'].dropna()\n",
    "\n",
    "for lat in latencies[1:]:\n",
    "    lat_qoe = df[df['latency_ms'] == lat]['quality_rating'].dropna()\n",
    "    d_qoe = cohens_d(baseline_qoe, lat_qoe)\n",
    "    print(f\"0ms vs {lat}ms: d = {d_qoe:.3f}\")\n",
    "\n",
    "    effect_sizes.append({\n",
    "        'outcome': 'QoE Quality',\n",
    "        'comparison': f'0ms vs {lat}ms',\n",
    "        'cohens_d': d_qoe,\n",
    "        'n_baseline': len(baseline_qoe),\n",
    "        'n_comparison': len(lat_qoe)\n",
    "    })\n",
    "\n",
    "# Save effect sizes\n",
    "effect_df = pd.DataFrame(effect_sizes)\n",
    "effect_df.to_csv(OUTPUT_DIR / 'effect_sizes.csv', index=False)\n",
    "print(f\"\\nEffect sizes saved to: {OUTPUT_DIR / 'effect_sizes.csv'}\")\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Effect Size Interpretation (Cohen's d):\")\n",
    "print(\"  Small: d = 0.2\")\n",
    "print(\"  Medium: d = 0.5\")\n",
    "print(\"  Large: d = 0.8\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Linear Mixed-Effects Models\n",
    "\n",
    "Properly account for within-subjects design with random effects for\n",
    "participants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LINEAR MIXED-EFFECTS MODELS\n",
      "================================================================================\n",
      "\n",
      "Model 1: Performance ~ Latency + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Mixed Linear Model Regression Results\n",
      "=======================================================\n",
      "Model:            MixedLM Dependent Variable: score_z  \n",
      "No. Observations: 488     Method:             REML     \n",
      "No. Groups:       27      Scale:              0.5674   \n",
      "Min. group size:  8       Log-Likelihood:     -582.9408\n",
      "Max. group size:  20      Converged:          Yes      \n",
      "Mean group size:  18.1                                 \n",
      "-------------------------------------------------------\n",
      "            Coef.  Std.Err.    z    P>|z| [0.025 0.975]\n",
      "-------------------------------------------------------\n",
      "Intercept    0.736    0.088   8.344 0.000  0.563  0.909\n",
      "latency_ms  -0.007    0.000 -16.388 0.000 -0.007 -0.006\n",
      "Group Var    0.121    0.059                            \n",
      "=======================================================\n",
      "\n",
      "\n",
      "Model 2: QoE Quality ~ Latency + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n",
      "           Mixed Linear Model Regression Results\n",
      "============================================================\n",
      "Model:            MixedLM Dependent Variable: quality_rating\n",
      "No. Observations: 487     Method:             REML          \n",
      "No. Groups:       27      Scale:              1.0052        \n",
      "Min. group size:  8       Log-Likelihood:     -715.2222     \n",
      "Max. group size:  20      Converged:          Yes           \n",
      "Mean group size:  18.0                                      \n",
      "-------------------------------------------------------------\n",
      "             Coef.   Std.Err.     z     P>|z|  [0.025  0.975]\n",
      "-------------------------------------------------------------\n",
      "Intercept     4.502     0.102   44.083  0.000   4.302   4.702\n",
      "latency_ms   -0.010     0.001  -17.863  0.000  -0.011  -0.009\n",
      "Group Var     0.124     0.050                                \n",
      "============================================================\n",
      "\n",
      "\n",
      "Model 3: QoE Quality ~ Latency + Performance + (1 | Participant)\n",
      "--------------------------------------------------------------------------------\n",
      "           Mixed Linear Model Regression Results\n",
      "============================================================\n",
      "Model:            MixedLM Dependent Variable: quality_rating\n",
      "No. Observations: 487     Method:             REML          \n",
      "No. Groups:       27      Scale:              0.9631        \n",
      "Min. group size:  8       Log-Likelihood:     -708.4202     \n",
      "Max. group size:  20      Converged:          Yes           \n",
      "Mean group size:  18.0                                      \n",
      "-------------------------------------------------------------\n",
      "             Coef.   Std.Err.     z     P>|z|  [0.025  0.975]\n",
      "-------------------------------------------------------------\n",
      "Intercept     4.318     0.114   37.782  0.000   4.094   4.542\n",
      "latency_ms   -0.008     0.001  -12.094  0.000  -0.009  -0.007\n",
      "score_z       0.250     0.059    4.208  0.000   0.133   0.366\n",
      "Group Var     0.151     0.059                                \n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LINEAR MIXED-EFFECTS MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Model 1: Performance (z-score) predicted by latency\n",
    "print(\"\\nModel 1: Performance ~ Latency + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "model1 = mixedlm(\n",
    "    \"score_z ~ latency_ms\",\n",
    "    data=df,\n",
    "    groups=df[\"participant_id\"]\n",
    ")\n",
    "result1 = model1.fit()\n",
    "print(result1.summary())\n",
    "\n",
    "# Save model summary\n",
    "with open(OUTPUT_DIR / 'mixed_model_performance.txt', 'w') as f:\n",
    "    f.write(str(result1.summary()))\n",
    "\n",
    "# Model 2: QoE Quality predicted by latency\n",
    "print(\"\\nModel 2: QoE Quality ~ Latency + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "qoe_valid = df.dropna(subset=['quality_rating'])\n",
    "model2 = mixedlm(\n",
    "    \"quality_rating ~ latency_ms\",\n",
    "    data=qoe_valid,\n",
    "    groups=qoe_valid[\"participant_id\"]\n",
    ")\n",
    "result2 = model2.fit()\n",
    "print(result2.summary())\n",
    "\n",
    "with open(OUTPUT_DIR / 'mixed_model_qoe.txt', 'w') as f:\n",
    "    f.write(str(result2.summary()))\n",
    "\n",
    "# Model 3: QoE Quality predicted by latency AND performance\n",
    "print(\"\\nModel 3: QoE Quality ~ Latency + Performance + (1 | Participant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "perf_qoe_valid = df.dropna(subset=['quality_rating', 'score_z'])\n",
    "model3 = mixedlm(\n",
    "    \"quality_rating ~ latency_ms + score_z\",\n",
    "    data=perf_qoe_valid,\n",
    "    groups=perf_qoe_valid[\"participant_id\"]\n",
    ")\n",
    "result3 = model3.fit()\n",
    "print(result3.summary())\n",
    "\n",
    "with open(OUTPUT_DIR / 'mixed_model_qoe_with_performance.txt', 'w') as f:\n",
    "    f.write(str(result3.summary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics Table\n",
    "\n",
    "Create a publication-ready summary table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUMMARY TABLE FOR PUBLICATION\n",
      "================================================================================\n",
      "            Performance Mean  Performance SD  QoE Mean  QoE SD  \\\n",
      "latency_ms                                                       \n",
      "0                      0.750           0.970     4.485   0.775   \n",
      "75                     0.279           0.800     3.793   0.962   \n",
      "150                   -0.309           0.786     3.026   1.297   \n",
      "225                   -0.720           0.726     2.321   1.155   \n",
      "\n",
      "            Acceptability %    N  \n",
      "latency_ms                        \n",
      "0                      96.7  122  \n",
      "75                     83.6  122  \n",
      "150                    59.5  121  \n",
      "225                    40.2  122  \n",
      "\n",
      "Summary table saved to:\n",
      "  - ../analysis/results/summary_table.csv\n",
      "  - ../analysis/results/summary_table.tex\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY TABLE FOR PUBLICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_table = df.groupby('latency_ms').agg({\n",
    "    'score_z': ['mean', 'std'],\n",
    "    'quality_rating': ['mean', 'std'],\n",
    "    'acceptable': ['mean', 'count']\n",
    "}).round(3)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_table.columns = [\n",
    "    'Performance Mean', 'Performance SD',\n",
    "    'QoE Mean', 'QoE SD',\n",
    "    'Acceptability %', 'N'\n",
    "]\n",
    "\n",
    "# Convert acceptability to percentage\n",
    "summary_table['Acceptability %'] = (summary_table['Acceptability %'] * 100).round(1)\n",
    "\n",
    "print(summary_table)\n",
    "\n",
    "# Save to CSV and LaTeX format\n",
    "summary_table.to_csv(OUTPUT_DIR / 'summary_table.csv')\n",
    "summary_table.to_latex(OUTPUT_DIR / 'summary_table.tex')\n",
    "\n",
    "print(f\"\\nSummary table saved to:\")\n",
    "print(f\"  - {OUTPUT_DIR / 'summary_table.csv'}\")\n",
    "print(f\"  - {OUTPUT_DIR / 'summary_table.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Statistical analysis complete! Key findings:\n",
    "\n",
    "1. **Correlation Analysis**: Examined relationships between latency and outcomes\n",
    "2. **ANOVA**: Tested significance of latency effects\n",
    "3. **Post-hoc Tests**: Identified which latency pairs differ significantly\n",
    "4. **Effect Sizes**: Quantified practical significance of latency impacts\n",
    "5. **Mixed Models**: Properly accounted for within-subjects design\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- Run `visualizations.ipynb` to create figures\n",
    "- Review all output files in `analysis/results/`\n",
    "- Interpret findings in the context of your research questions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
